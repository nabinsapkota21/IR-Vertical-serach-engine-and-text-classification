{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "static-registration",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-pioneer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T00:58:06.417024Z",
     "start_time": "2021-08-01T00:58:01.980987Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import string\n",
    "import json\n",
    "import nltk\n",
    "nltk.download('omw-1.4');\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virgin-capitol",
   "metadata": {},
   "source": [
    "# 1. Crawler Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-marks",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T00:58:06.423826Z",
     "start_time": "2021-08-01T00:58:06.420176Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define Seed Page\n",
    "URL = \"https://pureportal.coventry.ac.uk/en/organisations/coventry-university/persons/\"\n",
    "\n",
    "# Define profile URL format. This was obtained by manually examining the profile pages\n",
    "profile_url = \"https://pureportal.coventry.ac.uk/en/persons/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formal-shift",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T00:58:09.277766Z",
     "start_time": "2021-08-01T00:58:06.428114Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_maximum_page():\n",
    "    \n",
    "    first = requests.get(URL)\n",
    "    soup = BeautifulSoup(first.text, 'html.parser')\n",
    "    final_page = soup.select('#main-content > div > section > nav > ul > li:nth-child(12) > a')[0]['href']\n",
    "    fp = final_page.split('=')[-1]\n",
    "    return int(fp)\n",
    "    \n",
    "mx = get_maximum_page()\n",
    "mx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "systematic-mustang",
   "metadata": {},
   "source": [
    "web crawling for department of RCIH "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hidden-sugar",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T00:58:09.407285Z",
     "start_time": "2021-08-01T00:58:09.280073Z"
    }
   },
   "outputs": [],
   "source": [
    " #Check department\n",
    "def check_department(researcher):\n",
    "    \n",
    "    l1 = researcher.find('div', class_='rendering_person_short')\n",
    "      \n",
    "    for span in l1.find_all('span'):\n",
    "       \n",
    "        if span.text == str('Centre for Intelligent Healthcare'):\n",
    "            name = researcher.find('h3', class_='title').find('span').text\n",
    "            return name\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "def create_csv():\n",
    "     database = pd.DataFrame(columns=['Title', 'Author', 'Published', 'Link'])\n",
    "     database.to_csv('database.csv')\n",
    "    \n",
    "def update_csv(database):\n",
    "    current_data = pd.read_csv(database, index_col=\"Unnamed: 0\")\n",
    "    return current_data        \n",
    "\n",
    "def enter_each_researchers_publication(researcher, url, df):\n",
    "\n",
    "    new_url = url + str(researcher).replace(' ','-').lower() + '/publications/'\n",
    "    page = requests.get(new_url)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    results = soup.find(id=\"main-content\")\n",
    "    papers = results.find_all(\"li\", class_=\"list-result-item\")\n",
    "\n",
    "    for paper in papers:\n",
    "        title = paper.find('h3', class_='title')\n",
    "        if title is not None:\n",
    "            title_span = title.find('span')\n",
    "            title_text = title_span.text if title_span is not None else \"N/A\"\n",
    "        else:\n",
    "            title_text = \"N/A\"\n",
    "\n",
    "        author = paper.find('a', class_='link person')\n",
    "        print(author)\n",
    "        if author == None or author == 'None':\n",
    "            continue\n",
    "        else:\n",
    "            author_span = author.find('span')\n",
    "            author_text = author_span.text if author_span is not None else \"N/A\"\n",
    "\n",
    "        date = paper.find('span', class_=\"date\")\n",
    "        date_text = date.text if date is not None else \"N/A\"\n",
    "\n",
    "        link = paper.find('h3', class_='title')\n",
    "        link_href = link.find('a', href=True)['href'] if link is not None else \"N/A\"\n",
    "\n",
    "        # Read the existing database.csv file\n",
    "        opening = pd.read_csv('data.csv', index_col=\"Unnamed: 0\")\n",
    "\n",
    "        # Create a new DataFrame with the data to be appended\n",
    "        new_row = pd.DataFrame({'Title': [title.text],\n",
    "                                'Author': [author.text],\n",
    "                                'Published': [date.text],\n",
    "                                'Link': [link_href]})\n",
    "\n",
    "        # Concatenate the existing DataFrame and the new row DataFrame\n",
    "        opening = pd.concat([opening, new_row], ignore_index=True)\n",
    "\n",
    "        # Save the updated DataFrame to database.csv\n",
    "        opening.to_csv('data.csv')\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessory-customer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T00:58:09.442618Z",
     "start_time": "2021-08-01T00:58:09.431490Z"
    }
   },
   "outputs": [],
   "source": [
    "## Scrape function\n",
    "def scrape(mx):\n",
    "    df = update_csv('database.csv')\n",
    "    i=0\n",
    "    while True:\n",
    "    \n",
    "        if i > 17:\n",
    "            break\n",
    "            \n",
    "        if i>0:\n",
    "            url = URL + '?page=' + str(i)\n",
    "        else:\n",
    "            url = URL\n",
    "    \n",
    "        i = i+1\n",
    "        # scraping starts here\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "        results = soup.find(id=\"main-content\")\n",
    "        researchers = results.find_all(\"li\", class_=\"grid-result-item\")\n",
    "\n",
    "        for researcher in researchers:\n",
    "            # Check if researcher has any papers\n",
    "            check = researcher.find('div', class_='stacked-trend-widget')\n",
    "            if check:\n",
    "                name = check_department(researcher)\n",
    "                if name is None:\n",
    "                    pass\n",
    "                else:\n",
    "                    enter_each_researchers_publication(name, profile_url, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absent-vegetable",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T00:59:40.554457Z",
     "start_time": "2021-08-01T00:58:09.447859Z"
    }
   },
   "outputs": [],
   "source": [
    "#creating csv file\n",
    "create_csv()\n",
    "update_csv(database='data.csv') \n",
    "\n",
    "%time scrape(mx) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-subscription",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T00:59:40.569218Z",
     "start_time": "2021-08-01T00:59:40.557023Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_db = pd.read_csv('data.csv').rename(columns={'Unnamed: 0':'SN'})\n",
    "sample_db\n",
    "print(f'{sample_db.shape[0]} records were scraped')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "herbal-desktop",
   "metadata": {},
   "source": [
    "Indexing Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphic-potter",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T00:59:40.600384Z",
     "start_time": "2021-08-01T00:59:40.572032Z"
    }
   },
   "outputs": [],
   "source": [
    "scraped_db = pd.read_csv('data.csv').rename(columns={'Unnamed: 0':'SN'}).reset_index(drop=True)\n",
    "scraped_db.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-trauma",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T00:59:40.612357Z",
     "start_time": "2021-08-01T00:59:40.603287Z"
    }
   },
   "outputs": [],
   "source": [
    "single_row = scraped_db.loc[1,:].copy()\n",
    "single_row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valid-humor",
   "metadata": {},
   "source": [
    "Preprocess Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quiet-train",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T00:59:40.647183Z",
     "start_time": "2021-08-01T00:59:40.615161Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "sw = stopwords.words(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def tp1(txt):\n",
    "    # Make lowercase\n",
    "    txt = txt.lower()  \n",
    "    # Remove punctuation marks \n",
    "    txt = txt.translate(str.maketrans('','',string.punctuation))   \n",
    "    txt = lematize(txt)\n",
    "    return txt\n",
    "\n",
    "\n",
    "def fwpt(word):\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    hash_tag = {\"V\": wordnet.VERB, \"R\": wordnet.ADV,\"N\": wordnet.NOUN,\"J\": wordnet.ADJ}         \n",
    "    return hash_tag.get(tag, wordnet.NOUN)\n",
    "\n",
    "def lematize(text):\n",
    "        tkns = nltk.word_tokenize(text)\n",
    "        ax = \"\"\n",
    "        for each in tkns:\n",
    "            if each not in sw:\n",
    "                ax += lemmatizer.lemmatize(each, fwpt(each)) + \" \"\n",
    "        return ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fixed-triple",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T00:59:40.670745Z",
     "start_time": "2021-08-01T00:59:40.658588Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sample title\n",
    "single_row['Title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honey-strain",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T00:59:43.014244Z",
     "start_time": "2021-08-01T00:59:40.699046Z"
    }
   },
   "outputs": [],
   "source": [
    "# Demonstration of lowercase and punctuation removal\n",
    "tp1(single_row['Title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinated-columbus",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T00:59:43.027827Z",
     "start_time": "2021-08-01T00:59:43.016665Z"
    }
   },
   "outputs": [],
   "source": [
    "# Demonstration of lematization\n",
    "lematize(tp1(single_row['Title']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equipped-retrieval",
   "metadata": {},
   "source": [
    "#### Unprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-snapshot",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T07:25:01.875323Z",
     "start_time": "2021-08-01T07:25:01.858252Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_db['Title'].iloc[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "available-height",
   "metadata": {},
   "source": [
    "#### Processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocal-burton",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T07:25:27.562359Z",
     "start_time": "2021-08-01T07:25:27.557862Z"
    }
   },
   "outputs": [],
   "source": [
    "scraped_db['Title'].iloc[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-level",
   "metadata": {},
   "source": [
    "### 2.1.1 Preprocess entire dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "national-alexander",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T00:59:43.834681Z",
     "start_time": "2021-08-01T00:59:43.030064Z"
    }
   },
   "outputs": [],
   "source": [
    "processed_db = scraped_db.copy()\n",
    "\n",
    "def preprocess_df(df):\n",
    "    df.Title = df.Title.apply(tp1)\n",
    "    df.Author = df.Author.str.lower()\n",
    "    df = df.drop(columns=['Author','Published'], axis=1)\n",
    "    return df\n",
    "    \n",
    "preprocess_df(processed_db)\n",
    "processed_db.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offshore-driver",
   "metadata": {},
   "source": [
    "Index Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "insured-water",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T00:59:43.845672Z",
     "start_time": "2021-08-01T00:59:43.837012Z"
    }
   },
   "outputs": [],
   "source": [
    "single = processed_db.loc[0,:].copy()\n",
    "print(single)\n",
    "indexing_trial = {}\n",
    "\n",
    "words = single.Title.split()\n",
    "SN = single.SN\n",
    "word = words[0]\n",
    "example = {word: [SN]}\n",
    "\n",
    "print('=====================================================================')\n",
    "print('Sample index')\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-flour",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T00:59:43.854491Z",
     "start_time": "2021-08-01T00:59:43.848459Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knowing-amsterdam",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T00:59:44.790404Z",
     "start_time": "2021-08-01T00:59:43.857267Z"
    }
   },
   "outputs": [],
   "source": [
    "## Indexer Function\n",
    "def apply_index(inputs, index):\n",
    "    words = inputs.Title.split()\n",
    "    SN = int(inputs.SN)\n",
    "    for word in words:\n",
    "        if word in index.keys():\n",
    "            if SN not in index[word]:\n",
    "                index[word].append(SN)\n",
    "        else:\n",
    "            index[word] = [SN]\n",
    "    return index\n",
    "\n",
    "indx = apply_index(inputs=single, index= {})\n",
    "\n",
    "def full_index(df, index):\n",
    "    for x in range(len(df)):\n",
    "        inpt = df.loc[x,:]\n",
    "        ind = apply_index(inputs=inpt, index=index)\n",
    "    return ind\n",
    "\n",
    "def construct_index(df, index):\n",
    "    queue = preprocess_df(df)\n",
    "    ind = full_index(df=queue, index=index)\n",
    "    return ind\n",
    "\n",
    "indexed = full_index(processed_db, \n",
    "                     index = {})\n",
    "\n",
    "\n",
    "indexes = construct_index(df=scraped_db, \n",
    "                          index = {})\n",
    "\n",
    "with open('indexes.json', 'w') as new_f:\n",
    "    json.dump(indexes, new_f, sort_keys=True, indent=4)\n",
    "    \n",
    "with open('indexes.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "def index_2(df, x_path):\n",
    "    if len(df) > 0:\n",
    "        with open(x_path, 'r') as file:\n",
    "            prior_index = json.load(file)\n",
    "        new_index = construct_index(df = df, index = prior_index)\n",
    "        with open(x_path, 'w') as new_f:\n",
    "            json.dump(new_index, new_f, sort_keys=True, indent=4)\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-editing",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T23:13:44.430869Z",
     "start_time": "2021-08-01T23:13:44.285366Z"
    }
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-paintball",
   "metadata": {},
   "source": [
    "Query Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vanilla-football",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T11:02:51.119226Z",
     "start_time": "2021-08-01T11:02:51.112657Z"
    }
   },
   "outputs": [],
   "source": [
    "def demonstrate_query_processing():\n",
    "    sample = input('Enter Search Terms: ')\n",
    "    processed_query = tp1(sample)\n",
    "    print(f'Processed Search Query: {processed_query}')\n",
    "    return processed_query\n",
    "    \n",
    "def split_query(terms):\n",
    "    each = tp1(terms)\n",
    "    return each.split()\n",
    "\n",
    "dqp = demonstrate_query_processing()\n",
    "dqp\n",
    "print(f'Split Query: {split_query(dqp)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-postage",
   "metadata": {},
   "source": [
    "Boolean Functionalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ready-denial",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T01:00:26.010007Z",
     "start_time": "2021-08-01T01:00:26.004950Z"
    }
   },
   "outputs": [],
   "source": [
    "def union(lists):\n",
    "    union = list(set.union(*map(set, lists)))\n",
    "    union.sort()\n",
    "    return union\n",
    "\n",
    "def intersection(lists):\n",
    "    intersect = list(set.intersection(*map(set, lists)))\n",
    "    intersect.sort()\n",
    "    return intersect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "circular-distinction",
   "metadata": {},
   "source": [
    "\n",
    "Search Engine Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-pottery",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T01:00:26.020938Z",
     "start_time": "2021-08-01T01:00:26.012920Z"
    }
   },
   "outputs": [],
   "source": [
    "#Search engine function\n",
    "def vertical_search_engine(df, query, index=indexes):\n",
    "    query_split = split_query(query)\n",
    "    retrieved = []\n",
    "    for word in query_split:\n",
    "        if word in index.keys():\n",
    "            retrieved.append(index[word])              \n",
    "    # Ranked Retrieval\n",
    "    if len(retrieved)>0:\n",
    "        high_rank_result = intersection(retrieved)\n",
    "        low_rank_result = union(retrieved) \n",
    "        c = [x for x in low_rank_result if x not in high_rank_result]      \n",
    "        high_rank_result.extend(c)\n",
    "        result = high_rank_result\n",
    "        \n",
    "        final_output = df[df.SN.isin(result)].reset_index(drop=True)\n",
    "    \n",
    "        # Return result in order of Intersection ----> Union\n",
    "        dummy = pd.Series(result, name = 'SN').to_frame()\n",
    "        result = pd.merge(dummy, final_output, on='SN', how = 'left')   \n",
    "    else:\n",
    "        result = 'No result found'\n",
    "    \n",
    "    return result\n",
    "\n",
    "def test_search_engine():\n",
    "    xtest = scraped_db.copy()\n",
    "    query = input(\"Enter your search query: \")\n",
    "    return vertical_search_engine(xtest, query, indexed)\n",
    "    \n",
    "test_search_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accredited-crazy",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T11:43:05.137930Z",
     "start_time": "2021-08-01T11:43:05.131660Z"
    }
   },
   "outputs": [],
   "source": [
    "def final_engine(results):\n",
    "    if type(results) != 'list':\n",
    "        return results\n",
    "        #print(results)\n",
    "    else:\n",
    "        for i in range(len(results)):\n",
    "            printout = results.loc[i, :]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-astrology",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T11:46:39.943876Z",
     "start_time": "2021-08-01T11:46:39.927460Z"
    }
   },
   "outputs": [],
   "source": [
    "scraped_db['Author'].iloc[24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "speaking-savings",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T11:44:03.697525Z",
     "start_time": "2021-08-01T11:43:54.760859Z"
    }
   },
   "outputs": [],
   "source": [
    "final_engine(test_search_engine())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-toilet",
   "metadata": {},
   "source": [
    "Schedule Crawler for every week\n",
    "\n",
    "To demonstrate a weekly scheduled crawling, the following parameters are defined:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutting-prayer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T01:05:57.578325Z",
     "start_time": "2021-08-01T01:00:37.523485Z"
    }
   },
   "outputs": [],
   "source": [
    "days = 0\n",
    "interval = 7\n",
    "while days <= 1:\n",
    "    scrape(mx)\n",
    "    print(f\"Crawled at {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f'Next crawl scheduled after {interval} days')\n",
    "    time.sleep(interval)\n",
    "    days = days + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesser-biotechnology",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "processed-movement",
   "metadata": {},
   "source": [
    "Search Engine User Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patient-victim",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T01:15:13.914499Z",
     "start_time": "2021-08-01T01:15:13.885800Z"
    }
   },
   "outputs": [],
   "source": [
    "from tkinter import *\n",
    "from tkinter import messagebox\n",
    "from PIL import Image, ImageTk \n",
    "from tkinter import scrolledtext\n",
    "from pandastable import Table, TableModel\n",
    "from contextlib import suppress\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "image1 = Image.open('logo.png')\n",
    "resized_image1 = image1.resize((500,200), Image.ANTIALIAS)\n",
    "\n",
    "def new_gui(image1):\n",
    "    window = Tk()\n",
    "    window.configure(bg='#98AFC7')\n",
    "    window.title(\"IR Search Engine\")\n",
    "    window.geometry('1000x600')\n",
    "    \n",
    "    lbl1 = Label(window, text=\"Enter your query here\", bg=\"#87CEFA\",font=(\"Cambria\", 12), padx=5, pady=5)\n",
    "    lbl1.grid(column=0, row=1)\n",
    "    \n",
    "    img = ImageTk.PhotoImage(image1)\n",
    "    \n",
    "    lbl2 = Label(image=img)\n",
    "    lbl2.image = img\n",
    "    lbl2.grid(column=1, row=3, padx=5, pady=5)\n",
    "    \n",
    "    query = Entry(window, bg=\"#87CEFA\", font=(\"Cambria bold\", 12), width=60)\n",
    "    query.grid(column=1, row=1,  padx=5, pady=5)\n",
    "    \n",
    "    results = Canvas(window, height=30, width=600)\n",
    "    results.grid(column=1, row=2, padx=5, pady=5)\n",
    "    \n",
    "    # Entry\n",
    "    def getInputBoxValue():\n",
    "        userInput = query.get()\n",
    "        return userInput\n",
    "\n",
    "    # Button\n",
    "    def clicked():\n",
    "        search()\n",
    "           \n",
    "    def no_result():\n",
    "        messagebox.showwarning(\"Warning\", \"No results found. Please try again\")\n",
    "        \n",
    "    def search():\n",
    "        xtest = scraped_db.copy()\n",
    "        q = query.get()\n",
    "        f = Frame(window)\n",
    "        df = vertical_search_engine(xtest, q, indexed)\n",
    "        if type(df) == str:\n",
    "            no_result()\n",
    "        else:\n",
    "            pt = Table(results)\n",
    "            try:\n",
    "                table = pt = Table(results, dataframe=df)\n",
    "                pt.show()\n",
    "            except AttributeError:\n",
    "                pass\n",
    "            \n",
    "    def close_window():\n",
    "        if messagebox.askokcancel(\"Quit\", \"Quit Programme?\"):\n",
    "            window.destroy()\n",
    "        \n",
    "    btn = Button(window, text=\"Search\",font=(\"Cambria\", 12), bg=\"#87CEFA\", command=clicked)\n",
    "    btn.grid(column=2, row=1)\n",
    "    \n",
    "    window.protocol(\"WM_DELETE_WINDOW\", close_window)       \n",
    "    window.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ahead-spray",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T23:14:02.698938Z",
     "start_time": "2021-08-01T23:14:02.696390Z"
    }
   },
   "outputs": [],
   "source": [
    "new_gui(resized_image1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-butterfly",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
